0.配置apt-get中科大软件源
cd /etc/apt
sudo cp sources.list sources.list.bak
sudo vim sources.list
#在最底下加入：
deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse
deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse
deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
deb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse
deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse
deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
deb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
#保存并退出
sudo apt-get update





1.更新pip2
坑：直接pip install --upgrade pip 或者 pip2 install --upgrade pip2不行
解决方案：
sudo wget https://boostrap.pypa.io/get-pip.py
sudo puthon2.get-pip.py





2.装java
sudo apt-get install default-jdk
#所得文件目录在/usr/lib/jvm
#用户主目录可用echo $HOME显示
vim $HOME/.profile
#写入
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
#测试java
vim test.java
#输入
public class test{
	public static void main(String args[]){
		System.out.println("Hello world!");
	}
}
#保存并退出
javac test.java
java test
#如果顺利输出Hello world!则说明配置java成功





3.装hadoop
wget https://www.us.apache.org/dist/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz
tar -xzf hadoop-3.2.0.tar.gz
cd /etc/profile.d
sudo vim hadoop.sh
#写入
export HADOOP_HOME=....../hadoop-3.2.0
PATH=$PATH:$HADOOP_HOME/bin
#执行
. /etc/profile
#测试hadoop：
hadoop

###配置hadoop单机
#配置${HADOOP_HOME}/etc/hadoop/core-site.xml，在该文件中输入
<configuration>
        <property>
                <name>fs.default.name</name>
                <value>hdfs://localhost:9000</value>
        </property>
</configuration>

#配置${HADOOP_HOME}/etc/hadoop/hdfs-site.xml,在该文件输入
<configuration>
<!-- 这个参数设置为1，因为是单机版hadoop -->
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/home/michael/data/hadoop/data/namenode</value>
        </property>
        <property>
                <name>fs.checkpoint.dir</name>
                <value>/home/michael/data/hadoop/data/snn</value>
        </property>
        <property>
                <name>fs.checkpoint.edits.dir</name>
                <value>/home/michael/data/hadoop/data/snn</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/home/michael/data/hadoop/data/datanode</value>
        </property>
</configuration>

#配置${HADOOP_HOME}/etc/hadoop/mapred-site.xml，在该文件输入
<configuration>
    <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
   </property>
</configuration>

#配置/${HADOOP_HOME}/etc/hadoop/yarn-site.xml,在该文件输入
<configuration>
    <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
   </property>
</configuration>

#执行
. /etc/profile
hadoop namenode –format
cd ${HADOOP_HOME}/sbin

#启动
./start-dfs.sh
./start-yarn.sh

#若出现ssh: connect to host localhost port 22: Connection refused 
#说明ssh server没装
#检查命令如下，如果没有出现sshd则说明确实没装
ps -e |grep ssh
#安装sshd
sudo apt-get install openssh-server
#安装完后执行hadoop的启动命令会提示类似于localhost: Permission denied (publickey,password).的问题
#此时需要生成ssh密钥
ssh-keygen -t rsa
#一直按Enter，就会按照默认选项将生成的密钥对保存在 用户主目录/.ssh/id_rsa文件中
#进入.ssh目录
cp id_rsa.pub authorized_keys
ssh localhost
#再次执行hadoop启动代码
#如果出现localhost: ERROR: JAVA_HOME is not set and could not be found.的错误提示
cd ${HADOOP_HOME}/etc/hadoop
vim hadoop-env.sh
#添加JAVA_HOME,保存退出，再次执行hadoop启动文件

#访问本地网站
http://localhost:8088/

#关闭
./stop-dfs.sh
./stop-yarn.sh






4.装mysql
sudo apt-get install mysql-server
#登陆mysql
mysql -u root -p
#设置新密码为root（等号右边单引号里面设置新密码）
set password for 'root'@'localhost' = password('root')






5.装hive
#官网下载安装包：https://archive.apache.org/dist/hive/hive-3.0.0/apache-hive-3.0.0-bin.tar.gz
tar -xzf apache-hive-3.0.0-bin.tar.gz
cd etc/profile.d
sudo vim hive.sh
#写入
export HIVE_HOME=....../apache-hive-3.0.0-bin
PATH=$PATH:$HIVE_HOME/bin
###配置
#Step1:网上下载mysql-connector-java-版本号-bin.jar放到${HIVE_HOME}/lib下
#Step2:
cd ${HIVE_HOME}/conf
vim hive_env.sh
#写入
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
#保存退出
#Step3:配置hive-site.xml文件，如果不存在，创建之
vim hive-site.xml
#写入
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
                <description>JDBC connect string for a JDBC metastore</description>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
                <description>Driver class name for a JDBC metastore</description>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
                <description>username to use against metastore database</description>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>root</value>
                <description>password to use against metastore database</description>
        </property>
</configuration>

#执行
. /etc/profile
#启动hadoop
cd ${HADOOP_HOME}/sbin
./start-all.sh

#启动mysql
mysql -u root -p

#若没有初始化过数据库则执行：
schematool -dbType mysql -initSchema

#只要hadoop正常启动，一般都可以输入hive进入hive环境
#进入hive Shell
hive
#测试hive是否可用
CREATE TABLE x (a INT);
#返回类似于
#OK
#Time taken: 1.633 seconds
#的结果，则说明成功






6.装anaconda3
下载安装包
cd 到安装目录，默认为/media/下载/
chmod u+x Anaconda3-2018.12-Linux-x86_64.sh
./Anaconda3-2018.12-Linux-x86_64.sh
sudo vim /etc/profile
#写入：
export PATH=....../anaconda2/bin/:$PATH
#执行
. /etc/profile
#此时可用conda命令
#终端启动IDE可输入
spyder
#解决spyder无法输入中文的问题
cp /usr/lib/x86_64-linux-gnu/qt5/plugins/platforminputcontexts/libfcitxplatforminputcontextplugin.so ....../anaconda2/plugins/platforminputcontexts/

#配置python虚拟环境
conda create -n 环境名称 python=版本号(例如3.5) anaconda
#若提示权限不足之类的，用sudo对conda是无效的，需要将anaconda文件夹给当前用户
sudo chown -R 用户名 文件夹名
#激活环境
source activate 环境名称
#进入spyder编辑器，如果是win系统的话，进去spyder可能依然是base环境...
spyder





7.装tensorflow(CPU)
#下载安装包
https://mirror.tuna.tsinghua.edu.cn/tensorflow/linux/
#安装
sudo pip install 安装包




8.装pyltp
sudo pip install pyltp
#如果提示文件权限问题，赋权即可，安装时间较长，大约10分钟


9.装git
sudo apt-get install git
#获取git配置信息和配置git
git config --list
git config --global user.name ""
git config --global user.email ""
git config --global core.editor vim

#常用命令
git add 目录/文件
git status -s
git commit
git push origin master


10.装docker
#根据<<第一本Docker书>>
#添加Docker的ATP库
sudo sh -c "echo deb https://apt.dockerproject.org/repo ubuntu-trusty main > /etc/apt/surces.list.d/docker.list"
#添加Docker仓库的GPG秘钥
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
#更新APT源
sudo apt-get update
#安装Docker(龟速..)
sudo apt-get install docker-engine
#可能提示缺少依赖libsystemd-journal0
#可从https://debian.pkgs.org/8/debian-main-amd64/libsystemd-journal0_215-17+deb8u7_amd64.deb.html中下载
#其他缺失的直接sudo apt-get install xxx
#然后再安装Docker
#速度实在太慢的话直接梯子下
https://apt.dockerproject.org/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-trusty_amd64.deb
